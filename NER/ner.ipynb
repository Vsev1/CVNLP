{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T22:41:37.511933Z",
     "start_time": "2025-11-18T22:41:20.114508Z"
    }
   },
   "source": "pip install numpy pandas torch matplotlib transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForTokenClassification, AutoTokenizer"
   ],
   "id": "d789fcb76ac3bc83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the pre-trained model to use\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ],
   "id": "c50bf674a668330b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_dataset(filepath):\n",
    "    # Load dataset from a file in CoNLL-like format\n",
    "    final = []\n",
    "    sentences = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            # empty line indicates sentence boundary\n",
    "            if line == \"\\n\":\n",
    "                if len(sentences) > 0:\n",
    "                    final.append(sentences)\n",
    "                    sentences = []\n",
    "            else:\n",
    "                l = line.split(' ')\n",
    "                sentences.append((l[0], l[1].strip('\\n')))\n",
    "\n",
    "    return final\n",
    "\n",
    "# Load datasets\n",
    "train_samples = load_dataset('dataNER/train.txt')\n",
    "test_samples = load_dataset('dataNER/test.txt')\n",
    "valid_samples = load_dataset('dataNER/valid.txt')"
   ],
   "id": "dbfdc1aa7cbdc707"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "samples = train_samples + test_samples\n",
    "\n",
    "# Create the tag schema (all unique tags + padding '_')\n",
    "schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})"
   ],
   "id": "4101a9467f97f2be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_sample(sample):\n",
    "    # Convert each word to subtokens and keep its tag. It's a function which tokenize a single sample\n",
    "    seq = [\n",
    "        (subtoken, tag)\n",
    "        for token, tag in sample\n",
    "        for subtoken in tokenizer(token)['input_ids'][1:-1]\n",
    "    ]\n",
    "    # Add special tokens [CLS] (3) at start and [SEP] (4) at end with 'O' tag\n",
    "    return [(3, 'O')] + seq + [(4, 'O')]\n",
    "\n",
    "\n",
    "def preprocess(samples, schema):\n",
    "    # Map each tag to an index\n",
    "    tag_index = {tag: i for i, tag in enumerate(schema)}\n",
    "    # Tokenize all samples\n",
    "    tokenized_samples = list(map(tokenize_sample, samples))\n",
    "\n",
    "    # Find the maximum sequence length and initialize arrays for input IDs and labels\n",
    "    max_len = max(map(len, tokenized_samples))\n",
    "    X = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    y = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "\n",
    "    # Fill arrays with token IDs and label indices\n",
    "    for i, sentence in enumerate(tokenized_samples):\n",
    "        for j, (subtoken_id, tag) in enumerate(sentence):\n",
    "            X[i, j] = subtoken_id\n",
    "            y[i, j] = tag_index[tag]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, y_train = preprocess(train_samples, schema)\n",
    "X_test, y_test = preprocess(test_samples, schema)\n",
    "X_valid, y_valid = preprocess(valid_samples, schema)"
   ],
   "id": "a09c4ccc7fb83a62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert to PyTorch datasets\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.long),\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.long),\n",
    "    torch.tensor(y_test, dtype=torch.long),\n",
    ")\n",
    "valid_ds = TensorDataset(\n",
    "    torch.tensor(X_valid, dtype=torch.long),\n",
    "    torch.tensor(y_valid, dtype=torch.long),\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=8)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8)\n",
    "\n",
    "# Load model configuration and initialize model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(schema),\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            val_loss_total += outputs.loss.item()\n",
    "\n",
    "    val_loss = val_loss_total / len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Valid Loss: {val_loss:.4f}\")"
   ],
   "id": "8d497276001dc505"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "a4bbedd52e5e9fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation on validation set\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        logits = model(input_ids=input_ids).logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Only consider non-padding tokens\n",
    "        mask = labels != 0\n",
    "        correct += (preds[mask] == labels[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Final Accuracy: {accuracy:.3f}\")"
   ],
   "id": "60b1915543e66161"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T22:51:13.315131Z",
     "start_time": "2025-11-18T22:51:12.744281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_sentence(model, sentence, tokenizer, schema, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids).logits\n",
    "        predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    tags = [schema[p] for p in predictions]\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        print(f\"{token:15} -> {tag}\")\n",
    "\n",
    "print(\"\\nPrediction on Example 1:\")\n",
    "predict_sentence(model, \"I think Mount Everest looks beautiful\", tokenizer, schema, device)\n",
    "\n",
    "print(\"\\nPrediction on Example 2:\")\n",
    "predict_sentence(model, \"She came to France yesterday\", tokenizer, schema, device)"
   ],
   "id": "405eaba868fd3f0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction on Example 1:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mPrediction on Example 1:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mpredict_sentence\u001B[49m(model, \u001B[33m\"\u001B[39m\u001B[33mI think Mount Everest looks beautiful\u001B[39m\u001B[33m\"\u001B[39m, tokenizer, schema, device)\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mPrediction on Example 2:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m predict_sentence(model, \u001B[33m\"\u001B[39m\u001B[33mShe came to France yesterday\u001B[39m\u001B[33m\"\u001B[39m, tokenizer, schema, device)\n",
      "\u001B[31mNameError\u001B[39m: name 'predict_sentence' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd7d7f9fa873acb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "pip install rasterio opencv-python matplotlib torch torchvision torchaudio kornia kornia-moons"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "from rasterio.features import rasterize\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import kornia as K\n",
    "from kornia_moons.viz import draw_LAF_matches\n",
    "\n"
   ],
   "id": "23f1f08e1040bfde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:38:00.630879Z",
     "start_time": "2025-11-18T21:38:00.618056Z"
    }
   },
   "cell_type": "code",
   "source": "<span style=\"font-size:16px\">Download dataset from Kaggle (link: https://www.kaggle.com/datasets/isaienkov/deforestation-in-ukraine)</span>",
   "id": "fb701d722b7af3e5",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (173575708.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31m**Download dataset from Kaggle (link: https://www.kaggle.com/datasets/isaienkov/deforestation-in-ukraine)**\u001B[39m\n    ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get all raster JP2 files recursively from the data folder\n",
    "all_files_list = glob.glob(os.path.join(\"data\", \"**\", \"*_TCI.jp2\"), recursive=True)"
   ],
   "id": "8ba45d11984e5769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def open_raster_image(image):\n",
    "    # Open a raster image JP2 using rasterio and reshape to HWC format\n",
    "    with rasterio.open(image, \"r\", driver='JP2OpenJPEG') as src:\n",
    "        raster_image = src.read()\n",
    "        # raster_meta = src.meta\n",
    "    raster_image = reshape_as_image(raster_image)\n",
    "    return raster_image\n",
    "\n",
    "\n",
    "def show_raster_images(selected_images):\n",
    "    # Display multiple raster images in a 2x3 grid.\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 12))\n",
    "\n",
    "    for index, img in enumerate(selected_images):\n",
    "        row = index // 3\n",
    "        col = index % 3\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "images_ids = [0, 35, 15, 18, 21, 49]\n",
    "selected_images_paths = [all_files_list[i] for i in images_ids]\n",
    "\n",
    "selected_images = []\n",
    "for path in selected_images_paths:\n",
    "    img = open_raster_image(path)\n",
    "    height, width = img.shape[:2]\n",
    "    scale = 1024 / max(height, width)\n",
    "    resized_img = cv2.resize(img, (int(width * scale), int(height * scale)))\n",
    "    selected_images.append(resized_img)\n",
    "\n",
    "show_raster_images(selected_images)"
   ],
   "id": "f48e0482f2877f31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Matcher:\n",
    "    def __init__(self, image_size):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def match(self, image0, image1, confidence_min=0.8, accurate=False):\n",
    "        # Convert images to tensors\n",
    "        image0 = self._convert_image(image0)\n",
    "        image1 = self._convert_image(image1)\n",
    "\n",
    "        # Convert images to grayscale for LoFTR input\n",
    "        input_dict = {\n",
    "            'image0': K.color.rgb_to_grayscale(image0),\n",
    "            'image1': K.color.rgb_to_grayscale(image1)\n",
    "        }\n",
    "\n",
    "        # Initialize pretrained LoFTR matcher (outdoor dataset)\n",
    "        matcher_model = K.feature.LoFTR(pretrained='outdoor').eval()\n",
    "        with torch.inference_mode():\n",
    "            corresp = matcher_model(input_dict)\n",
    "\n",
    "        # Create a mask to select keypoints with confidence above the threshold\n",
    "        mask = corresp['confidence'] > confidence_min\n",
    "\n",
    "        # Apply the mask to filter keypoints and confidence\n",
    "        kpts0 = corresp['keypoints0'][mask].cpu().numpy()\n",
    "        kpts1 = corresp['keypoints1'][mask].cpu().numpy()\n",
    "        confidence = corresp['confidence'][mask].cpu().numpy()\n",
    "\n",
    "        # Compute fundamental matrix and inliers\n",
    "        fmat, inliers = cv2.findFundamentalMat(kpts0, kpts1, cv2.USAC_ACCURATE, 1, 0.99, 100000)\n",
    "        inliers = inliers > 0\n",
    "\n",
    "        # Return results as a dictionary\n",
    "        results = {'image0': image0, 'image1': image1, 'keypoints0': kpts0, 'keypoints1': kpts1,\n",
    "                   'confidence': confidence, 'inliers': inliers}\n",
    "\n",
    "        return results\n",
    "\n",
    "    def show_keypoints_matches(self, feature_matches):\n",
    "        # Convert numpy keypoints to tensors\n",
    "        keypoints0 = torch.from_numpy(feature_matches['keypoints0']).unsqueeze(0)\n",
    "        keypoints1 = torch.from_numpy(feature_matches['keypoints1']).unsqueeze(0)\n",
    "\n",
    "        num_points0 = keypoints0.shape[1]\n",
    "        num_points1 = keypoints1.shape[1]\n",
    "\n",
    "        # Set default scale and orientation for lafs\n",
    "        scales0 = torch.ones(1, num_points0, 1, 1)\n",
    "        scales1 = torch.ones(1, num_points1, 1, 1)\n",
    "        orients0 = torch.ones(1, num_points0, 1)\n",
    "        orients1 = torch.ones(1, num_points1, 1)\n",
    "\n",
    "        # Generate LAFs from keypoints\n",
    "        laf0 = K.feature.laf_from_center_scale_ori(keypoints0, scales0, orients0)\n",
    "        laf1 = K.feature.laf_from_center_scale_ori(keypoints1, scales1, orients1)\n",
    "\n",
    "        # Prepare matches as consecutive index pairs\n",
    "        matches = torch.arange(num_points0).unsqueeze(1).repeat(1, 2)\n",
    "\n",
    "        # Draw matches using Kornia visualization\n",
    "        output_figure = draw_LAF_matches(\n",
    "            laf0,\n",
    "            laf1,\n",
    "            matches,\n",
    "            K.tensor_to_image(feature_matches['image0']),\n",
    "            K.tensor_to_image(feature_matches['image1']),\n",
    "            feature_matches['inliers'],\n",
    "            draw_dict={\n",
    "                'inlier_color': (0.2, 1, 0.2),\n",
    "                'tentative_color': (1, 0.1, 0.1),\n",
    "                'feature_color': (0.2, 0.5, 1),\n",
    "                'vertical': False\n",
    "            }\n",
    "        )\n",
    "        return output_figure\n",
    "\n",
    "    def _convert_image(self, input_image):\n",
    "        # convert image to tensor and normalize it to [0,1]. After that resize it\n",
    "        tensor_img = K.utils.image_to_tensor(input_image).float()\n",
    "        new_image = tensor_img.unsqueeze(0) / 255.0\n",
    "        resized_img = K.geometry.resize(new_image, self.image_size, interpolation='area')\n",
    "        return resized_img\n",
    "\n",
    "# Load images as numpy arrays\n",
    "model_images = [open_raster_image(path) for path in selected_images_paths]\n",
    "\n",
    "# Initialize matcher with target image size\n",
    "image_size = (1024, 1024)\n",
    "matcher = Matcher(image_size)\n",
    "\n",
    "# Show matches between the second and third selected images\n",
    "show_matches(model_images[1], model_images[2], matcher)"
   ],
   "id": "24d8df3d6c4ace92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
